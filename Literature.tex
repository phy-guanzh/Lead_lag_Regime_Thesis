%% ----------------------------------------------------------------
%% Conclusions.tex
%% ---------------------------------------------------------------- 
\chapter{Literature Review} \label{Chapter: Conclusions}
\section{Non-stationarity in Financial Markets}

Non-stationarity is one of the typical characteristics in financial market, and a considerable literature has studied its implications for modelling and inference.  this review around the premise that asset-return dynamics are non-stationary: conditional means, variances, and dependence structures evolve over time. Four strands are central to this view: (i) covariance non-stationarity as a source of heavy tails, (ii) adaptive market dynamics that render predictability state- and time-dependent, (iii) cross-asset time-series momentum as evidence of time-varying serial dependence, and (iv) regime-switching volatility. This organizing principle also guides the empirical design that follows.

\cite{Schmitt_2013} put non-stationarity at the center of their analysis. Using 306 continuously traded S\&P 500 stocks from 1992–2012, they show in rolling windows that volatilities and correlations vary strongly over time; when the covariance matrix is held fixed over short windows, multivariate returns look approximately Gaussian. To capture the fact that covariances themselves fluctuate, they replace the fixed covariance with a Wishart random matrix, deriving an ensemble-averaged return distribution governed by a single parameter $\textbf{N}$ that measures the strength of correlation/covariance fluctuations. Smaller $\textbf{N}$ means stronger non-stationarity and heavier tails; as $\textbf{N} \rightarrow \infty$, the model approaches Gaussian behaviour. Empirically, the model fits the market-wide return distribution well and attributes heavy tails to time-varying covariances. Estimated $\textbf{N}$ rises with the return horizon—about $\textbf{N} \approx \textbf{5}$ for daily returns and $\textbf{N} \approx \textbf{14}$ for 20-day returns—implying non-stationarity is stronger at short horizons. Although some deviations remain in the extreme tails and the underlying economic mechanisms are not addressed, the framework provides a consistent way to capture heavy tails through covariance non-stationarity.

\cite{Lo} places non-stationarity at the core of the Adaptive Markets Hypothesis: market efficiency, the risk–return relation, the equity risk premium, and strategy profitability all change over time with the market “ecology” (participants, institutions, regulation), so empirical relationships are time-varying and path-dependent rather than fixed. In this framework, arbitrage opportunities emerge and disappear, and strategies wax and wane with shifting conditions, replacing the EMH’s stationary convergence with cycles, trends, and episodes of panic and recovery. Consistent with non-stationarity, a rolling five-year first-order autocorrelation of the S\&P Composite (1871–2003) shows cyclical efficiency rather than a monotonic march to zero autocorrelation, challenging stationary-equilibrium implementations of EMH. Practically, the model implies that adaptation and innovation are necessary responses to a changing environment and that survival—not static optimality—is the organizing objective. Finally, behavioral regularities are interpreted as evolutionary heuristics whose market impact varies with population composition, with emotion and selection shaping who remains active—further evidence that the data-generating process is non-stationary.

Using 58 liquid futures/forwards across equities, FX, commodities, and government bonds (mainly 1985–2009) and scaling by ex-ante volatility for comparability, \cite{MOSKOWITZ2012228} document strong 1–12-month continuation followed by partial multi-year reversal. A diversified TSMOM factor delivers large alphas relative to standard equity and “everywhere” factors and performs best in extreme markets, indicating state-dependent payoffs. A formal decomposition attributes profits mainly to positive auto-covariance (time-series dependence), with cross-asset lead–lag and mean-return components small or even negative. Position data further show speculators ride trends while hedgers take the other side. Taken together, these patterns support a non-stationary, state-dependent return-generating process rather than one with constant parameters.


\cite{HAMILTON1994307}replace a stationary GARCH view with a Markov-switching ARCH (SWARCH) specification in which volatility parameters jump across latent regimes, estimated on weekly NYSE value-weighted returns (1962–1987). Allowing 2–4 regimes and Student-t innovations (with leverage), they show that standard GARCH overstates persistence and forecasts poorly, whereas SWARCH captures discrete regime shifts—quiet, moderate, high, and an extreme state that isolates the October 1987 crash—and improves short- and multi-week variance forecasts (notably for the 4-state model). Persistence is reinterpreted as state persistence (long-lived regimes) rather than slow decay of shocks; the ARCH component itself is much less persistent once regime changes are modeled. High-volatility states line up with business recessions, and negative returns raise volatility more than positive ones of equal size (leverage), further underscoring time-variation in the data-generating process. Overall, the paper reframes equity volatility as a regime-switching, non-stationary phenomenon, yielding cleaner inference and better forecasts than stationary ARCH/GARCH benchmarks.
%Financial markets are widely recognised as non-stationary systems, meaning that their statistical properties—such as volatility, correlations, and return distributions—change over time rather than remaining constant~\cite{Schmitt_2013, Lo}. This feature distinguishes financial data from many traditional time series models that assume stationarity, and it poses a fundamental challenge for both trading and risk management.



\section{Approaches to Regime Detection}

\cite{10.1093/rfs/15.4.1137} pursue a parametric Markov-switching approach in which regimes are latent states of a multivariate normal DGP with state-dependent means, volatilities, and correlations; regimes and transition probabilities are inferred via the Hamilton/Gray filter and evaluated with the Regime Classification Measure (RCM) based on smoothed state probabilities. This specification yields economically interpretable states—most notably a persistent “bear” regime with higher volatility and higher cross-market correlations and a “normal” regime with lower risk—together with expected regime durations and transition matrices that can be used for forecasting and portfolio choice. To validate the classification, they show the RS model reproduces asymmetric exceedance correlations in downturns, where Gaussian and asymmetric GARCH benchmarks do not, and they examine robustness to equal-means, imperfectly synchronized regimes, and RS-ARCH volatilities. The payoff of this route is clear interpretability and tractable forecasting; the trade-offs are reliance on parametric structure (including often constant transition probabilities) and potential misspecification if the true state space is richer than a finite-state Markov chain.

\cite{issa2023nonparametriconlinemarketregime} develop a non-parametric, online method for market regime detection and clustering on path space, built on a path-wise two-sample test using maximum mean discrepancy (MMD) with rough-path signatures (the signature kernel) as features. 
 The framework targets small, streaming batches for fast reactivity, scales to multidimensional series, and handles path-dependence/non-Markovian structure; a rank-2 variant incorporates filtration (conditional) information, and a kernel-trick implementation avoids signature truncation. The same machinery supports regime clustering and outlier detection and is demonstrated on synthetic data and real markets (equity baskets, crypto), where the algorithms swiftly flag historical turmoil and offer a fully automated pipeline for MRDP/MRCP tasks.

\cite{chevyrev2025primersignaturemethodmachine} offer a concise primer on the signature method, defining a path’s signature as an infinite sequence of iterated integrals that compactly captures its geometry and serves as a nonparametric feature map for sequential data. They develop core properties—invariance to time reparametrization, the shuffle product, Chen’s identities, time reversal, and the log-signature—and situate signatures within rough-path theory and related questions such as path uniqueness and moment problems. Building on these foundations, they lay out a practical pipeline—data → path → signature → features → learning—and show how signatures support supervised and unsupervised tasks; an illustrative pen-digit example highlights that second-order (area-type) terms improve class separability, and the approach extends naturally to multivariate time series (e.g., finance). Finally, linear functionals of signature terms approximate continuous functionals on compact path sets, providing a principled basis for using (often linear) models on signature features.

\section{Lead–Lag Signals and Regime-Driven Overlay Trading Strategies}

\cite{BROOKS200131} provide a high-frequency study of the FTSE 100 spot--futures lead--lag using 10-minute data (13{,}035 observations, Jun 1996--1997). They confirm cointegration via Engle--Granger and estimate an error-correction model (ECM) alongside a cost-of-carry–augmented version (ECM-COC) that incorporates the risk-free rate and dividends; ARMA and VAR serve as benchmarks. The futures series is built from the nearest contract (rolled on the 10th of the expiry month), and timestamps are aligned by taking the last bid/ask within each 10-minute window, with futures quotes constrained to precede the paired spot print to avoid mechanical “lead” bias. Cointegration holds and lagged futures returns significantly forecast spot returns; the carry-adjusted cointegration term is also significant. In a May 1997 hold-out, ECM-COC achieves the best out-of-sample performance (correct-direction rate 68.75\%, lowest RMSE/MAE). Turning forecasts into trades, they test several rules (every-signal liquidation, conditional buy-and-hold, and filter thresholds): without costs, monthly returns reach 15.62\% versus 4.09\% for a passive benchmark, but under realistic round-trip costs (spot $\approx$ 1.70\%, futures $\approx$ 0.116\%) plus a 10-minute execution delay, all active rules fail to outperform. Overall, the results support price discovery led by futures and highlight that frictions and latency largely dissipate exploitable profits, suggesting the models are more useful for timing or low-cost market-making contexts than for retail trading.

\cite{huth2012leadlag} present an empirical study of high-frequency lead–lag using tick-by-tick Reuters data for CAC40 stocks and index futures (Mar–May 2010), measured via the Hayashi–Yoshida cross-correlation and an asymmetric “lead–lag ratio” (LLR). Simulations show that previous-tick estimators spuriously ascribe leadership to more active assets, whereas Hayashi–Yoshida avoids this sampling bias. Empirically, futures lead constituents with sub-second lags and the strongest asymmetry (future/stock LLR $\approx$ 2), stock/stock links are weaker with $\approx$ 1-second peak lags, and a minimum-spanning-tree exposes sectoral structure for selecting leader–lagger pairs. Leadership aligns with liquidity—shorter intertrade durations, narrower spreads, lower midquote volatility, higher turnover—while the most highly correlated pairs tend to have similar liquidity. The effect varies intraday: it intensifies around macro releases (14:30, 16:00 CET) and the U.S. open (15:30 CET). Conditioning on large moves further strengthens lead/lag, and using only the leader’s past attains ~60\% accuracy for predicting the lagger’s next midquote change; nonetheless, naive market-order strategies fail to clear the bid–ask spread. Altogether, the paper documents robust microsecond-to-seconds price-discovery asymmetries and offers practical diagnostics for forecasting and pairs selection. 

\cite{bennett2022leadlag} propose an unsupervised pipeline for detecting lead–lag clusters in multivariate time series: (i) build a directed network from pairwise lead–lag scores—either functionals of cross-correlation over multiple lags (Pearson/Kendall/distance correlation/mutual information; “ccf-lag1” and “ccf-auc”) or a signatures-based area term from rough paths—and (ii) extract communities with high inter-cluster flow imbalance via Hermitian spectral clustering (with DI-SIM and bibliometric symmetrisation as baselines). They define a meta-flow matrix and a simple “leadingness” row-sum ranking, and discuss computational scaling. Synthetic factor-style experiments (linear, cosine, Legendre, Hermite, heterogeneous) show that ccf-auc with non-linear dependence measures plus Hermitian/DI-SIM clustering recovers ground-truth communities with high ARI at low noise, while naive symmetrisation degrades and signature/lag-1 metrics falter under negative associations. Applied to 434 high-volume U.S. equities (CRSP daily closes, 2000–2019), using ccf-auc with distance correlation and lags $l\in{-5,\dots,5}$ and targeting $k=10$ clusters, they uncover statistically significant lead–lag structure (permutation test on the top Hermitian eigenvalue, $p<0.005$), clusters not reducible to sectors (though Mining and Finance appear relatively leading), and meta-flow edges stronger than industry-based partitions; the learned clusters yield a statistically significant predictive trading signal. Overall, the study shows that directed-network clustering of multi-lag dependence can surface persistent leader–lagger communities and usable forecasts in noisy, high-dimensional markets.

\cite{lu2025tugofwar} study a market-wide “tug of war” between overnight speculation and daytime correction by decomposing daily returns into overnight and daytime components, building directed, signed lead–lag networks from cross-stock correlations (overnight$\to$daytime and daytime$\to$overnight), and using these to trade only across disjoint leader/lagger groups. They introduce a specialized spectral clustering method, d-LE-SC, derived from a directed SBM–likelihood with flow-based objectives, to extract leader and lagger communities and then form long–short portfolios that take signals from leaders and trade within laggers—thereby isolating cross-stock spillovers from within-stock autocorrelation. In U.S. equities (CRSP; 2000–01–03 to 2024–12–31), the overnight-lead–daytime strategy delivers an annualized return of 32.11\% with Sharpe 2.37 and Calmar 1.84, outperforming the reverse (daytime-lead–overnight: 15.79\%, Sharpe 2.09) and a close-to-close benchmark (23.35\%, Sharpe 1.56); alphas remain statistically significant across CAPM, FF3/FF5, and momentum-augmented models. Cross-stock signals are economically distinct from firm-level tug-of-war reversals: correlations are low, exposures differ in sign, and cross-stock portfolios retain strength in recent years while close-to-close and firm-level reversals decay. Robustness checks (alternative clustering baselines, Pearson vs.\ Spearman dependence) support the findings; turnover is manageable and, while transaction costs attenuate performance, the cross-stock overnight$\to$daytime edge persists, highlighting networked price discovery and correction that extend beyond individual stocks.

