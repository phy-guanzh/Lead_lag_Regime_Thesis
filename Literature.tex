%% ----------------------------------------------------------------
%% Conclusions.tex
%% ---------------------------------------------------------------- 

\chapter{Literature Review} \label{Chapter: Conclusions}

\section{Non-stationarity in Financial Markets}

Non-stationarity is a defining feature of financial returns. In practice, the conditional mean, the variance, the higher moments, and the cross-sectional dependence structure evolve over time as market composition, policy, and technology change. Accordingly, this subsection organises the literature into four strands that motivate our empirical design. First, time-varying covariances can generate heavy tails. Second, adaptive market dynamics imply state- and time-dependent predictability. Third, cross-asset time-series momentum provides evidence of time-varying serial dependence. Fourth, volatility often switches across regimes. Consistent with these strands, we emphasise approaches that model non-stationarity explicitly, such as Markov switching and time-varying volatility. We also emphasise approaches that infer non-stationarity non-parametrically via rolling windows and clustering. Finally, we relate these ideas to our later use of \emph{path-wise} signature features and strict walk-forward clustering for regime identification.


\cite{Schmitt_2013} place non-stationarity at the center of their analysis. The authors study 306 continuously traded S\&P 500 stocks from 1992--2012. In rolling windows, they show that volatilities and correlations vary strongly over time. When the covariance matrix is held fixed over short windows, multivariate returns appear approximately Gaussian. To capture the fact that covariances fluctuate, the authors replace the fixed covariance with a Wishart random matrix. This substitution yields an ensemble-averaged return distribution governed by a single parameter $N$, which measures the strength of correlation and covariance fluctuations. A smaller $N$ indicates stronger non-stationarity and heavier tails. As $N \rightarrow \infty$, the model approaches Gaussian behaviour. Empirically, the model fits the market-wide return distribution well and attributes heavy tails to time-varying covariances. The estimated $N$ rises with the return horizon. The estimate is about $N \approx 5$ for daily returns and about $N \approx 14$ for 20-day returns. This pattern implies that non-stationarity is stronger at short horizons. However, some deviations remain in the extreme tails. The study does not address the underlying economic mechanisms. Even so, the framework provides a consistent way to capture heavy tails through covariance non-stationarity.


\cite{Lo} places non-stationarity at the core of the Adaptive Markets Hypothesis (AMH). In this view, market efficiency changes over time as the market ecology evolves. Moreover, the risk-return relation changes as conditions shift. Likewise, the equity risk premium and strategy profitability change with the environment. Specifically, the ecology includes participants, institutions, and regulation, which shift continually. Consequently, empirical relationships are time-varying and path-dependent rather than fixed. Within this framework, arbitrage opportunities emerge and disappear. Strategies wax and wane as conditions change. Thus, the AMH replaces the EMH's stationary convergence with cycles, trends, and episodes of panic and recovery. For evidence, a rolling five-year first-order autocorrelation for the S\&P Composite (1871-2003) shows cyclical efficiency. The pattern does not march monotonically toward zero autocorrelation. Hence, the result challenges stationary-equilibrium implementations of the EMH. Practically, the model implies that adaptation and innovation are necessary responses to a changing environment. Accordingly, survival rather than static optimality becomes the organising objective. Finally, the AMH interprets behavioural regularities as evolutionary heuristics whose market impact varies with population composition. Moreover, emotion and selection shape which agents remain active. Taken together, these features provide additional evidence that the data-generating process is non-stationary.


Similarly, \cite{MOSKOWITZ2012228} study 58 liquid futures and forwards across equities, FX, commodities, and government bonds. The sample covers mainly 1985–2009. For comparability, the authors scale positions by ex-ante volatility. Consequently, the authors document strong 1-12-month continuation followed by partial multi-year reversal. Moreover, a diversified TSMOM factor delivers large alphas relative to standard equity factors and ``everywhere'' factors. The factor performs best in extreme markets. Hence, the payoffs appear state-dependent. In a formal decomposition, the authors attribute profits mainly to positive auto-covariance, which reflects time-series dependence. By contrast, the cross-asset lead-lag and mean-return components are small or even negative. In addition, position data show that speculators ride trends, whereas hedgers take the other side. Finally, these patterns support a non-stationary, state-dependent return-generating process rather than one with constant parameters.


In line with the above evidence on non-stationarity, \cite{HAMILTON1994307} replace a stationary GARCH view with a Markov-switching ARCH (SWARCH) specification. The specification lets volatility parameters jump across latent regimes. The authors estimate the model on weekly NYSE value-weighted returns from 1962 to 1987. The model allows 2-4 regimes and student's $t$ innovations with a leverage term. The results show that standard GARCH overstates persistence and forecasts poorly. By contrast, SWARCH captures discrete regime shifts. The shifts include quiet, moderate, high, and an extreme state that isolates the October 1987 crash. The model improves short- and multi-week variance forecasts, especially for the four-state case. The paper also reframes persistence. The authors interpret persistence as state persistence, meaning long-lived regimes, rather than the slow decay of shocks. Once regime changes are modelled, the ARCH component becomes much less persistent. High-volatility states line up with business recessions. Negative returns raise volatility more than equal-size positive returns, which confirms leverage. These patterns further underscore time variation in the data-generating process. Overall, the paper treats equity volatility as a regime-switching, non-stationary phenomenon. Consequently, the framework yields cleaner inference and better forecasts than stationary ARCH/GARCH benchmarks.

Taken together, these strands support an empirical design. The design treats regimes as evolving objects and detects regimes from data rather than imposing them \emph{ex ante}. Accordingly, our approach uses signature-based path features. Our approach also applies strict walk-forward clustering. Together, these tools yield daily bull, neutral, and bear states. We then condition a cross-sectional lead-lag hedge on those states.

\section{Path Signatures for Sequential Data}

A \emph{path signature} is an infinite, ordered list of iterated integrals\cite{issa2023nonparametriconlinemarketregime}. The signature captures both the shape and the time ordering of a multivariate path. The concept comes from rough path theory. \cite{Lyons1998} formalise the signature as a unique encoding up to time reparameterisation. The representation is non-parametric and coordinate-free.
For a path $X:[0,T]\to\mathbb{R}^d$, the $k$-th level is
\[
S^{(k)}(X)
=\int_{0<t_1<\cdots<t_k<T} dX_{t_1}\otimes\cdots\otimes dX_{t_k}.
\]
This term is a tensor in $(\mathbb{R}^d)^{\otimes k}$. The level collects all $k$-fold, time-ordered integrals. These levels encode increments, signed areas, and higher-order interactions. Consequently, the signature summarises how the path evolves through time, not only where it ends. The signature is invariant to the speed of traversal, and also obeys key algebraic laws. The \emph{shuffle product} and \emph{Chen's identity} link products and concatenations of paths \cite{https://doi.org/10.1112/plms/s3-4.1.502}. These laws give the object a rich and tractable structure. A truncation at order $m$ yields a finite feature vector. The vector summarises the path up to order-$m$ effects. Thus, the signature provides practical features with clear theoretical meaning.

Over the past decade, path signatures have moved to the forefront of machine learning for sequential data. Lyons and collaborators have led this shift. A growing body of work shows that signature features are powerful inputs for time-series classification and prediction. For example, \cite{chevyrev2025primersignaturemethodmachine} provides a concise primer. The authors highlight reparameterisation invariance, the existence of a log-signature in a free Lie algebra, and a universal approximation view. Consequently, the properties justify using signatures as a basis for feature learning. In practice, standard pipelines follow three steps. One first embeds raw discrete observations into a continuous path, for example via piecewise-linear interpolation. One then computes a truncated signature. The approximation result provides the key rationale. Polynomial functionals of a path reduce to linear functionals of signature terms. Therefore, a linear model on sufficiently high-order signature features can approximate any continuous functional on a suitable path space. This universality allows complex path dependence without manual feature engineering. Instead, the model learns weights on signature terms.


For financial applications, the second-order signature terms represent \emph{areas} spanned by pairs of variables over time. These signed areas encode temporal ordering information. Consequently, researchers use these areas as measures of lead-lag or causal asymmetry between time series. For instance, if an asset $A$ consistently loops ahead of another asset $B$ in price-path space, the signed area becomes large. This pattern implies that $A$ tends to lead $B$. Moreover, \cite{bennett2022leadlag} include a rough-path signature \emph{area} term in their lead--lag correlation measures. Likewise, \cite{repec:taf:quantf:v:17:y:2017:i:6:p:959-977} proposes a hypothesis test for lag/lead relations based on the statistical significance of the signed area between two time series. Taken together, these applications show that path signatures detect subtle time-directed dependencies. Standard correlation or cross-covariance methods often miss such effects.


Another advantage is that signatures combine naturally with kernel methods. In particular, the \emph{signature kernel} defines an inner product between signature feature maps. The kernel enables efficient comparison of paths in a high-dimensional feature space. The approach avoids explicit truncation of the signature. Building on this idea, \cite{issa2023nonparametriconlinemarketregime} develop an online two-sample test for regime shifts. The test uses signature features as the basis for a maximum mean discrepancy (MMD) statistic. Consequently, the method can flag, and flag quickly, when the distribution of recent market data departs from that of the past. More broadly, signature features capture fine-grained sequential structure and coarse aggregate effects. Therefore, the approach suits non-stationary financial problems.Accordingly, the remainder of this thesis will use path signature features via the signature kernel. We will embed asset price trajectories into a feature space. We will identify regimes and measure lead--lag effects in that space. Finally, we will remain agnostic about specific functional forms of dependence while exploiting the properties above.


\section{Approaches to Regime Detection}

Regime detection methods broadly fall into two families. \emph{Parametric} models posit a finite set of latent states with state-dependent moments and Markovian transitions. As a result, these models yield smoothed or filtered state probabilities and transition matrices that aid forecasting. By contrast, \emph{non-parametric} approaches infer state changes directly from the data's geometry. These approaches typically use rolling windows, distances or kernels, and clustering. Therefore, non-parametric methods relax distributional and Markov assumptions and also facilitate online updates. In our review, we compare methods along five dimensions. We consider modelling assumptions, estimation mode (batch vs.\ online and leakage control), output type (hard labels vs.\ probabilities), scalability to multivariate paths, and interpretability for portfolio use.

\cite{10.1093/rfs/15.4.1137} specify regimes as latent states of a multivariate normal DGP with state-dependent means, volatilities, and correlations. The authors infer regimes and transition probabilities via the Hamilton/Gray filter. The authors summarise classification quality with the Regime Classification Measure (RCM). The model recovers economically interpretable states; for example, the model finds a persistent high-volatility, high-correlation bear state and a lower-risk normal state. The model also reproduces downturn exceedance correlations where Gaussian and asymmetric GARCH fail. Key strengths include probabilistic outputs, expected durations, and tractable forecasts. However, key trade-offs include reliance on parametric structure, often with constant transition probabilities, and misspecification risk when the true dynamics deviate from a finite-state Markov chain.

\cite{issa2023nonparametriconlinemarketregime} develop an online regime detector on \emph{path space} using a two-sample MMD with rough-path signatures as features. The method uses the \emph{signature kernel} to compare paths. The pipeline ingests small streaming batches for fast reactivity. The pipeline scales to multidimensional series and handles non-Markovian, path-dependent structure. A rank-2 variant incorporates filtration (conditional) information. A kernel-trick implementation avoids explicit signature truncation. The same machinery also supports clustering and outlier detection. The demonstrations cover synthetic data and real markets, including equity baskets and crypto. The main appeal is minimal modelling assumptions and online operation. The main cost is that outputs are often hard labels or distance-based scores without an explicit transition model.

\cite{chevyrev2025primersignaturemethodmachine} provide a concise primer on path signatures. The authors describe signatures as an infinite sequence of iterated integrals that compactly capture time ordering. Core properties include time-reparametrisation invariance, the shuffle product, Chen's identities, time reversal, and the log-signature. These properties justify signatures as non-parametric feature maps for sequential data. A practical pipeline follows a simple flow: data $\rightarrow$ path $\rightarrow$ signature $\rightarrow$ features $\rightarrow$ learning. In many cases, second-order (area-type) terms improve class separability and extend naturally to multivariate time series. Linear functionals of signature terms approximate continuous functionals on compact path sets. Hence, signature features provide a principled basis for often linear models.

Guided by the above evidence, we adopt a non-parametric and \emph{strictly walk-forward} design. Our procedure embeds BTC paths via the signature kernel. Our rolling group distances then feed a clustering step on \emph{eligible history} only. Our mapping step assigns clusters to bull, neutral, or bear using forward $H$-day effects. Our voting step converts hard labels into a daily series via a $k$-vote. Compared with Markov-switching models, our design avoids parametric transition assumptions and emphasises leakage control. Compared with generic online detectors, our design targets portfolio use by tying cluster semantics to forward returns and by conditioning a cross-sectional lead--lag hedge on the detected state.


\section{Clustering-Based Regime Identification}

Beyond model-based switching frameworks, researchers identify market regimes with \emph{unsupervised clustering} of historical data. These approaches partition time periods into groups with internally similar characteristics. The characteristics often include return distributions and volatility levels. These methods do not require an explicit likelihood or a Markov structure. The main advantage is that clustering makes minimal assumptions about dynamics. As a result, clustering can detect novel or irregular regimes. The main challenge is that clustering typically produces hard classifications with no inherent temporal continuity. Unless researchers add an exogenous transition process, the labels do not evolve smoothly. Nevertheless, prior studies show that clustering can uncover economically meaningful states in financial markets.

One influential example is the Wasserstein $k$-means algorithm proposed by \cite{Horvath_2021, Horvath2024JCF}. This method treats each time window as a distribution. This method then clusters windows using the $p$-Wasserstein distance between empirical return distributions. This design contrasts with classical $k$-means, which clusters raw returns by Euclidean distance. Because the method considers the full distributional shape, the method is robust to heavy tails and skewness. The authors show that Wasserstein $k$-means can detect distinct market phases in an unsupervised fashion. The authors also show that the method significantly outperforms classical $k$-means and other baselines in separating regimes. The resulting clusters correspond to intuitive states, such as low-volatility and high-volatility regimes. Importantly, the method achieves this separation without a parametric state model. Follow-up work validates the method by computing maximum mean discrepancy scores between clusters. These studies confirm that the distributional differences are statistically significant. In both simulations and real financial time series, the Wasserstein clustering distinguishes regime segments reliably. In addition, the method adapts more flexibly than models that assume Gaussian returns or fixed transition probabilities.

Clustering methods can also incorporate cross-sectional information. \cite{10.1007/978-3-031-12426-6_1} argue that many regime detectors cannot identify new regimes on the fly or ignore relationships across assets. The authors propose a dynamic clustering model that jointly learns regime assignments for multiple time series while allowing time-varying transition probabilities. The idea is to group time periods by common cross-asset behaviour rather than by univariate patterns. This design yields cross-sectional regimes that improve multi-asset return forecasts. In this framework, regimes emerge as clusters of high-dimensional observations across assets. A flexible transition mechanism then captures how these clusters evolve. This line of work underscores the breadth of clustering approaches. The spectrum runs from simple rolling-window similarity checks to sophisticated high-dimensional models. The shared goal is to let the data speak for itself about regime structure.

Another related strand is non-parametric change-point detection. Researchers can view this strand as clustering with implicit two-segment clusters, namely pre-break and post-break. \cite{Matteson2014} develop an e-divisive means algorithm to detect multiple change points in multivariate series without parametric distributional assumptions. The method identifies points where the distribution shifts and thereby segments the series into distinct regimes. These techniques complement clustering. Whereas clustering often groups windows that need not be contiguous in time, change-point methods explicitly seek contiguous regime segments. Practitioners can also combine the two ideas by first detecting candidate breakpoints and then clustering segments.

Clustering-based methods typically output hard labels rather than smoothed probabilities. This property yields clear ex-post identification of regimes. However, this property also requires care in online or trading contexts to avoid lookahead bias. Many studies perform clustering on an entire historical sample or use rolling re-training that inadvertently uses future information. For practical trading signals, a strict walk-forward implementation is necessary. Such an implementation must use only information that is available at the decision time. Our design follows this principle by using an expanding window for clustering. Our design does not re-label past data when new data arrives. Our design defines regimes via forward-looking returns on a training set and then applies those definitions prospectively.

Clustering methods generally do not attach economic semantics to regimes by default. Users usually attach semantics by examining cluster characteristics, such as high volatility or large drawdowns, and then assigning names. Our approach bridges this gap by mapping clusters to bull, neutral, or bear labels based on forward $H$-day return outcomes in a training period. This mapping injects an economic interpretation directly into the clustering output. Thus, our approach forms a hybrid. The hybrid combines unsupervised identification of patterns with a supervised labeling step based on subsequent performance. This blend, together with strict walk-forward operation, aims to harness the adaptability of clustering while making the results usable in a live strategy.

\section{Lead--Lag Signals and Regime-Driven Overlay Trading Strategies}

Lead--lag captures a \emph{directed, time-shifted} dependence between assets. Specifically, the empirical evidence comes from two main strands. The first strand covers high-frequency studies that document futures-led price discovery and microstructure asymmetries. The second strand examines daily, multi-lag dependence that researchers organise as directed networks and convert into predictive signals. A recurring constraint in this literature is the role of \emph{trading frictions and execution latency}. These frictions determine whether statistical leadership translates into net returns. Accordingly, we summarise representative studies and we connect them to our regime-driven overlay design.

\cite{BROOKS200131} study FTSE~100 futures and cash with 10-minute data from 1996--1997. The authors first establish cointegration via Engle--Granger. The authors then estimate an error-correction model (ECM) and a cost-of-carry variant (ECM--COC) against ARMA and VAR benchmarks. Lagged futures returns significantly forecast spot returns. The carry-adjusted cointegration term is also significant. In a May 1997 hold-out, ECM--COC delivers the best out-of-sample accuracy with 68.75\% correct direction and the lowest RMSE and MAE. However, realistic round-trip costs (spot $\approx 1.70\%$, futures $\approx 0.116\%$) and a 10-minute execution delay remove the edge. Therefore, the key takeaway is clear. \emph{Futures lead price discovery, but frictions and latency can exhaust exploitable profits}.

\cite{huth2012leadlag} analyse CAC40 stocks and index futures with tick-by-tick data from March to May 2010. The authors use the \emph{Hayashi--Yoshida} correlation and an asymmetric lead--lag ratio (LLR). The simulations show that previous-tick estimators spuriously assign leadership to more active assets. The Hayashi--Yoshida estimator mitigates this bias. The empirical results show that futures lead constituents at sub-second lags with strong asymmetry (future/stock LLR $\approx 2$). The stock--stock links are weaker with peak lags near one second. The leadership aligns with liquidity through shorter intertrade durations, narrower spreads, lower midquote volatility, and higher turnover. The leadership also strengthens around macro releases and the U.S.\ open. A simple predictor that uses only the leader’s past yields about 60\% accuracy for the lagger’s next midquote change. Nevertheless, naïve market-order strategies do not clear the spread. Hence, the evidence supports a second conclusion. \emph{Leadership--liquidity co-moves are robust, but profitability is fee- and latency-constrained}.

\cite{bennett2022leadlag} propose an unsupervised pipeline for daily data. The authors construct a directed network from pairwise lead--lag scores. The scores include cross-correlation functionals across lags (Pearson, Kendall, distance correlation, and mutual information; \emph{ccf-lag1} and \emph{ccf-auc}). The scores also include a rough-path \emph{signature} area term. The authors then extract communities with high flow imbalance via Hermitian spectral clustering, with DI-SIM and bibliometric symmetrisation as baselines. On synthetic factors and on 434 U.S.\ equities (CRSP daily closes, 2000--2019), \emph{ccf-auc} with non-linear dependence plus Hermitian or DI-SIM recovers ground-truth communities with high ARI. The procedure uncovers leading clusters that are not reducible to sectors. The procedure also passes a permutation test on the top Hermitian eigenvalue ($p<0.005$). The learned structure yields a statistically significant trading signal. Thus, the results show that \emph{daily data also contain stable directed structure}. However, the results remain sensitive to the metric and to the symmetrisation step.

\cite{lu2025tugofwar} decompose daily returns into overnight and daytime components. The authors build directed and signed networks for overnight$\to$daytime and daytime$\to$overnight links. The authors introduce d-LE-SC, which derives from a directed SBM likelihood with flow objectives, to extract leader and lagger communities. The portfolios trade within laggers by using signals from leaders. The design isolates cross-stock spillovers from within-stock autocorrelation. In U.S.\ equities (CRSP; 2000--2024), the overnight$\to$daytime strategy delivers a 32.11\% annualised return with a Sharpe ratio of 2.37 and a Calmar ratio of 1.84. The strategy outperforms the reverse link and a close-to-close benchmark. The alphas remain significant under CAPM, FF3/FF5, and momentum-augmented models. The turnover is manageable, and the costs attenuate but do not eliminate performance. Therefore, the results highlight \emph{networked price discovery and correction} beyond single-stock reversals.

Taken together, the literature suggests two practical lessons. The first lesson is that leadership is strongest at high frequency but remains fragile to frictions. The second lesson is that, at daily horizons, multi-lag directed dependence can be organised into communities and used for prediction. Building on these insights, we operate at \emph{daily} frequency and we use \emph{signature} features and construct an \emph{antisymmetric} lead--lag matrix whose row means rank leaders and followers. We implement a baseline hedge that goes long followers and short leaders with next-day execution. We then add a \emph{regime-driven overlay}. We route assets by the sign of their BTC-anchored relation and by strictly walk-forward signature--kernel regime labels, and tilt toward co-movers in bull regimes, flip in bear regimes, scale down in neutral states, and preserve the hedge at all times. Consequently, our design uses \emph{state information} to mitigate mismatch and left-tail risk. At the same time, our design emphasises \emph{leakage control} through strict walk-forward operation, ties cluster semantics to forward $H$-day returns, and integrates regimes directly into portfolio construction.


\section{Research Gap}\label{sec:lit:gap}

The literature reports extensive evidence on non-stationarity. The literature also documents a growing body of work on lead--lag structure. However, an framework that couples path-wise regime detection with a deployable lead--lag trading overlay remains underdeveloped. This gap is especially visible in cryptocurrency markets.

First, on regime detection, parametric Markov-switching models offer interpretable state probabilities and transition matrices. However, these models rely on restrictive distributional and Markov assumptions that may shift across episodes. By contrast, non-parametric and online approaches on path space relax these assumptions. Yet these approaches rarely deliver an \emph{economic} mapping that directly informs portfolio decisions. In particular, prior studies seldom link clusters to forward returns in a strictly walk-forward manner. Moreover, many unsupervised classifiers are applied to full samples or recalibrated with hindsight. Such practices create problems for live implementation.

Second, on lead--lag estimation and trading, the strongest leadership effects appear at high frequency. Microstructure frictions and latency often erase profits in those settings. Daily-horizon studies document usable directed dependence in equities via multi-lag networks. However, the evidence in crypto markets remains limited. The use of \emph{signature-based, antisymmetric} scores that encode temporal order beyond linear correlation also remains rare. Furthermore, the idea of routing cross-sectional positions by a \emph{signed relation to an anchor} (for example, BTC) is largely unexplored. Such anchors may plausibly concentrate market-wide propagation.

Third, at the strategy--design interface, most papers develop regime signals or lead--lag signals \emph{in isolation}. The literature lacks regime-\emph{aware} overlays that preserve a market-neutral hedge while reassigning names dynamically by state. A practical overlay would favour co-movers in bull regimes. A practical overlay would flip in bear regimes. A practical overlay would scale in neutral periods. The overlay would also hold \emph{the same market participation}. Without this control, many comparisons conflate signal quality with changes in gross exposure or time in market. Such conflation obscures the true source of improvements.

Finally, many evaluation protocols underplay practical constraints. Several studies do not enforce strict walk-forward retraining. Several studies also do not align signals to next-day execution and treat costs only superficially. These gaps limit external validity and deployability.

This dissertation addresses these gaps with three steps. First, we construct strict walk-forward, signature-kernel regime labels whose semantics tie to forward $H$-day returns. Second, we estimate a signature-based, antisymmetric lead--lag matrix and form a simple followers-long/leaders-short hedge. Third, we overlay the hedge with regime-aware routing via a BTC-anchored signed relation while preserving market neutrality and aligning signals to $t^+$ execution under explicit transaction-cost scenarios. Remaining open questions include probabilistic (soft) regime labels, joint multi-asset regime estimation, and cost-aware optimisation that endogenises turnover. These questions define directions for future work.
